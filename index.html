<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>My Resume</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
  <style>
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
      font-family: 'Inter', sans-serif;
    }
    body {
      background-color: #f7f9fc;
      color: #333;
      padding: 0;
      line-height: 1.6;
      display: flex;
      justify-content: center;
      font-size: 0.85rem; /* Set global font size */
    }
    .wrapper {
      max-width: 1000px;
      width: 100%;
      background: #fff;
      padding: 2rem 3rem;
    }
    nav {
      display: flex;
      gap: 1.5rem;
      margin-bottom: 2rem;
    }
    nav a {
      text-decoration: none;
      color: #2b2d42;
      font-weight: 600;
      padding: 0.3rem 0.6rem;
      border-radius: 6px;
    }
    nav a:hover {
      background-color: #eaeaea;
    }
    h1, h2 {
      color: #2b2d42;
      font-size: 1.4rem; /* Adjusted font size for headings */
    }
    h1 {
      font-size: 1.8rem; /* Heading size */
      margin-bottom: 1rem;
    }
    section {
      margin-bottom: 2rem;
    }
    .news-box {
      height: 200px;
      overflow-y: auto;
      border: 1px solid #ddd;
      border-radius: 8px;
      padding: 1rem;
      background: #fdfdfd;
    }
    .news-box::-webkit-scrollbar {
      width: 6px;
    }
    .news-box::-webkit-scrollbar-thumb {
      background-color: #ccc;
      border-radius: 6px;
    }
    ul {
      padding-left: 1.2rem;
    }
    li {
      margin-bottom: 0.5rem;
    }
    .subtitle {
      font-weight: 600;
      margin-bottom: 0.5rem;
    }
    .publication {
      display: flex;
      align-items: flex-start;
      margin-bottom: 1.5rem;
      gap: 20px;
    }
    .paper-box {
      display: flex;
      gap: 20px;
      align-items: center;
      width: 100%;
    }
    .paper-box-image {
      position: relative;
    }
    .image-container {
      position: relative;
    }
    .badge {
      background-color: #3b5998;
      color: white;
      padding: 0.4rem 0.8rem;
      font-weight: bold;
      border-radius: 5px;
      font-size: 0.8rem;
      position: absolute;
      top: 0px;
      left: 0px;
    }

    .paper-box-text {
      max-width: 800px;
      flex: 1;
    }

    .paper-box-text strong {
      font-size: 0.95rem; /* Adjusted size for strong text */
      color: #2b2d42;
    }

    .paper-box-text em {
      font-style: italic;
      color: #666;
    }

    .paper-box-text ul {
      padding-left: 1.2rem;
    }

    footer {
      text-align: center;
      font-size: 0.8rem;
      color: #999;
      margin-top: 3rem;
    }
    #about {
  display: flex;
  flex-direction: column; /* Ensure the heading is above the image and text */
  margin-bottom: 2rem;
}

.about-container {
  display: flex;
  gap: 20px; /* Add space between image and text */
  align-items: center; /* Vertically center the content */
}

.about-image {
  flex-shrink: 0; /* Prevent the image from shrinking */
}

.about-text {
  flex: 1; /* Allow text to take up remaining space */
}

.about-image img {
  width: 150px;
  height: auto;
  border-radius: 8px;
  border: 2px solid #ddd;
}

#experience {
  margin-bottom: 2rem;
}

.experience-container {
  display: flex;
  flex-wrap: wrap; /* Allow items to wrap to the next line */
  gap: 20px; /* Space between logo and content */
}

/* Container for logos */
.logo-container {
  flex-shrink: 0; /* Prevent the logo from shrinking */
  width: 100px; /* Fixed width for the logo */
  height: 100px; /* Set a fixed height */
  display: flex;
  justify-content: center; /* Center the logo horizontally */
  align-items: center; /* Center the logo vertically */
}

.logo {
  height: 100%; /* Maintain fixed height for logo */
  width: auto; /* Let the width scale based on the height */
  object-fit: contain; /* Ensures the logo fits within the container without distortion */
}

.experience-items {
  flex: 1; /* Take up remaining space */
  min-width: 300px; /* Ensure content takes up some space even on smaller screens */
}

.experience-item {
  margin-bottom: 1rem;
}

.subtitle {
  font-weight: 600;
  margin-bottom: 0.5rem;
}

/* Change color of all links to a steady red */
a {
  color: #9b2d20; /* Subtle red color */
  text-decoration: none; /* Remove underline for a cleaner look */
}

a:hover {
  color: #b23c2b; /* Slightly lighter red on hover */
  text-decoration: underline; /* Add underline on hover */
}
.contact-links {
    margin-top: 20px;
    display: flex;
    justify-content: flex-start;
    align-items: center;
  }

  .contact-links .icon {
    margin-right: 20px;
  }

  .contact-links a {
    font-size: 16px;
    color: #333;
    text-decoration: none;
    display: flex;
    align-items: center;
  }

  .contact-links a:hover {
    color: #007BFF;
  }
  </style>
</head>
<body>
  <div class="wrapper">
    <nav>
      <!-- <a href="#about">About Me</a> -->
      <a href="#research-interests">Research Interests</a>
      <a href="#news">News</a>
      <a href="#experience">Experience</a>
      <a href="#publications">Publications</a>
      <a href="#honors">Honors</a>
      <a href="#service">Service</a>
    </nav>

    <h1>Dongming Wu</h1>
 
    <section id="about">
      <!-- <h2>About Me</h2> -->
      <div class="about-container">
        <div class="about-image">
          <img src="./images/photo.jpg" alt="Dongming Wu" style="width: 150px; border-radius: 8px; border: 2px solid #ddd;" />
        </div>
        <div class="about-text">
          <p>
            I am currently a Postdoctoral Fellow at MMLab, the Chinese University of Hong Kong, and at CPII, working with Prof. Xiangyu Yue. <br>
            <br>
            In 2025.06, I received my PhD degree in Department of Computer Science, Beijing Institute of Technology, advised by <a href="https://scholar.google.com/citations?user=_Q3NTToAAAAJ&hl=en">Prof. Jianbing Shen</a>. <br>
            In 2019.06, I received my Bachelor degree from the Class of Xu at the same university.
          </p>
        </div>
      </div>

  <!-- Contact Links Section -->
  <div class="contact-links">
    <span class="icon">
      <a class="text-dark" href="https://github.com/wudongming97" target="_blank">
        <i class="fab fa-github mx-2" style="font-size: 30px;"></i> GitHub
      </a>
    </span>
    <span class="icon">
      <a class="text-dark" href="https://scholar.google.com/citations?user=ejFCAq0AAAAJ&hl=zh-CN" target="_blank">
        <i class="fab fa-google mx-2" style="font-size: 30px;"></i> Google Scholar
      </a>
    </span>
    <span class="icon">
      <a class="text-dark" href="mailto:wudongming97@gmail.com">
        <i class="fas fa-envelope mx-2" style="font-size: 30px;"></i> Gmail
      </a>
    </span>
  </div>
    </section>


    <section id="research-interests">
      <h2>Research Interests</h2>

      My current research interests lie in vision-language learning, multimodal large language models (MLLMs), and embodied agents. 
      (1) During my graduate studies, I focused on building intelligent perception models that understand visual and linguistic information. 
      (2) More recently, I‚Äôve been exploring decision-making systems capable of actively interacting with both humans and dynamic environments. 
      Ultimately, my goal is to develop human-like agents that can perceive real-world environments and make autonomous decisions, moving us closer to achieving artificial general intelligence (AGI). 
      Two articles that have deeply inspired my thinking are <a href="https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf">The Bitter Lesson</a> and <a href="https://ysymyth.github.io/The-Second-Half/">The Second Half</a>.<br>
      <br>
      <strong>I am always open to collaboration and discussions about the latest advancements in the field. Feel free to reach out!</strong>
    </section>

    <section id="news">
      <h2>News</h2>
      <div class="news-box">
        <ul>
          <li>üéâ 2025.06: One paper (RAGNet) is accepted by ICCV2025.</li>
          <li>üéì 2025.06: I successfully defense my Ph.D. thesis. I‚Äôm awarded Outstanding Graduates of Beijing (Âåó‰∫¨Â∏Ç‰ºòÁßÄÊØï‰∏öÁîü).</li>
          <li>2025.02: One paper (DrivingSphere) is accepted by CVPR2025.</li>
          <li>2024.12: One paper (NuPrompt) is accepted by AAAI2025.</li>
          <li>2024.07: One paper (Merlin) is accepted by ECCV2024.</li>
          <li>2024.05: I‚Äôm awarded Excellent Doctoral Thesis Seedling Fund (‰ºòÁßÄÂçöÂ£´ËÆ∫ÊñáËÇ≤ËãóÂü∫Èáë).</li>
          <li>2024.01: One paper (TopoMLP) is accepted by ICLR2024.</li>

        </ul>
      </div>
    </section>

    <section id="experience">
      <h2>Experience</h2>
      <div class="experience-container">
        <div class="logo-container">
          <img src="images/logo_dexmal.png" alt="Logo" class="logo" />
        </div>
        <div class="experience-items">
          <div class="experience-item">
            <p class="subtitle">Dexmal</p>
            <p>Research Intern</p>
            <p>Mentor: <a href="https://scholar.google.com.hk/citations?user=pF9KA1sAAAAJ&hl=zh-CN">Yingfei Liu</a> and <a href="https://scholar.google.com.hk/citations?user=YI0sRroAAAAJ&hl=zh-CN">Tiancai Wang</a></p>
          </div>
        </div>
        <div class="logo-container">
          <img src="images/logo_mbzuai.png" alt="Logo" class="logo" />
        </div>
        <div class="experience-items">
          <div class="experience-item">
            <p class="subtitle">MBZUAI</p>
            <p>Visiting Student</p>
            <p>Mentor: <a href="https://mbzuai.ac.ae/study/faculty/rao-muhammad-anwer/">Prof. Rao Muhammad Anwer</a> and <a href="https://sites.google.com/view/fahadkhans">Prof. Fahad Shahbaz Khan</a></p>
          </div>
        </div>
        <div class="logo-container">
          <img src="images/logo_megvii.jpeg" alt="Logo" class="logo" />
        </div>
        <div class="experience-items">
          <div class="experience-item">
            <p class="subtitle">MEGVII</p>
            <p>Research Intern</p>
            <p>Mentor: <a href="https://scholar.google.com.hk/citations?user=YI0sRroAAAAJ&hl=zh-CN">Tiancai Wang</a> and <a href="https://scholar.google.com/citations?user=yuB-cfoAAAAJ&hl=zh-CN&oi=ao">Xiangyu Zhang</a></p>
          </div>
        </div>
        <div class="logo-container">
          <img src="images/logo_IIAI.png" alt="Logo" class="logo" />
        </div>
        <div class="experience-items">
          <div class="experience-item">
            <p class="subtitle">IIAI</p>
            <p>Research Intern</p>
            <p>Mentor: <a href="https://xingpingdong.github.io/">Xingping Dong</a> and <a href="https://scholar.google.com/citations?user=z84rLjoAAAAJ&hl=zh-CN&oi=ao">Prof. Ling Shao</a></p>
          </div>
        </div>
      </div>
    </section>
    

    <section id="publications">
      <h2>Publications</h2>
      <h3>Preprint Papers</h3>
      <ul>
        <li>
          <strong>Grounding Beyond Detection: Enhancing Contextual Understanding in Embodied 3D Grounding</strong><br>
          Yani Zhang, Dongming Wu, Hao Shi, Yingfei Liu, Tiancai Wang, Haoqiang Fan, Xingping Dong<br>
          <a href="https://huggingface.co/spaces/AGC2024/visual-grounding-2024" target="_blank"> Rank 1st place on EmbodiedScan Visual Grounding</a><br>
          | 2025 | 
          <a href="https://arxiv.org/abs/2506.05199" target="_blank">Paper</a> | 
          <a href="https://github.com/zyn213/DEGround" target="_blank">Code</a> |
        </li>
        <hr>
      
        <li>
          <strong>Is a 3D-Tokenized LLM the Key to Reliable Autonomous Driving?</strong><br>
          Yifan Bai*, Dongming Wu*, Yingfei Liu, Fan Jia, Weixin Mao, Ziheng Zhang, Yucheng Zhao, Jianbing Shen, Xing Wei, Tiancai Wang, Xiangyu Zhang (*Equal Contributions)<br>
          | 2024 | 
          <a href="https://arxiv.org/pdf/2405.18361" target="_blank">Paper</a> |
        </li>
        <hr>

      
        <li>
          <strong>Bootstrapping Referring Multi-Object Tracking</strong><br>
          Yani Zhang, Dongming Wu, Wencheng Han, Xingping Dong<br>
          | 2024 | 
          <a href="https://arxiv.org/pdf/2406.05039" target="_blank">Paper</a> | 
          <a href="https://github.com/zyn213/TempRMOT" target="_blank">Code</a> |
        </li>
        <hr>
      </ul>
      <br>

      <h3>Conference Papers</h3>
        <div class="paper-box">
          <div class="paper-box-image">
            <div class="image-container">
              <div class="badge">ICCV 2025</div>
              <img src="images/ragnet.jpg" alt="sym" style="width: 300px;" />
            </div>
          </div>
          <div class="paper-box-text">
            <strong>RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping</strong><br>
            | ICCV 2025 | 
            <a href="https://arxiv.org/abs/2507.23734" target="_blank">Paper</a> | 
            <a href="https://github.com/wudongming97/AffordanceNet" target="_blank">Code</a> |<br>
            Dongming Wu, Yanping Fu, Saike Huang, Yingfei Liu, Fan Jia, Nian Liu, Feng Dai, Tiancai Wang, Rao Muhammad Anwer, Fahad Shahbaz Khan, Jianbing Shen<br>
            <ul>
              <li><strong style="color:green; font-size: 0.8rem;">We present a large-scale reasoning-based affordance segmentation benchmark RAGNet and introduce a comprehensive affordance-based grasping framework AffordanceNet.</strong></li>
            </ul>
          </div>
        </div>
        <hr>


      <div class='paper-box'>
        <div class='paper-box-image'>
          <div class="image-container">
            <div class="badge">CVPR 2025</div>
            <img src="images/drivingsphere.jpg" alt="sym" style="width: 300px;" />
          </div>
        </div>
        <div class='paper-box-text' markdown="1">
          <strong>DrivingSphere: Building a High-fidelity 4D World for Closed-loop Simulation</strong><br>
          | CVPR 2025 |
          <a href="https://arxiv.org/abs/2411.11252" target="_blank">Paper</a> | 
          <a href="https://yanty123.github.io/DrivingSphere/" target="_blank">Code</a> |<br>
          Tianyi Yan, Dongming Wu, Wencheng Han, Junpeng Jiang, Xia Zhou, Kun Zhan, Cheng-zhong Xu, Jianbing Shen
          <ul>
            <li><strong style="color:green; font-size: 0.8rem;">DrivingSphere is a novel geometry-aware closed-loop simulation framework that captures 2D visual and 3D geometric properties while seamlessly integrating with vision-based end-to-end driving agents.</strong></li>
          </ul>
        </div>
      </div>
      <hr>



      <div class='paper-box'>
        <div class='paper-box-image'>
          <div class="image-container">
            <div class="badge">AAAI 2025</div>
            <img src="images/nuprompt.jpg" alt="sym" style="width: 300px;" />
          </div>
        </div>
        <div class='paper-box-text' markdown="1">
          <strong>Language prompt for autonomous driving</strong><br>
          | AAAI 2025 | 
          <a href="https://arxiv.org/pdf/2309.04379.pdf" target="_blank">Paper</a> | 
          <a href="https://github.com/wudongming97/Prompt4Driving" target="_blank">Code</a> |<br>
          Dongming Wu, Wencheng Han, Tiancai Wang, Yingfei Liu, Xiangyu Zhang, Jianbing Shen <br>
          <ul>
          <li><strong style="color:green; font-size: 0.8rem;">DrivingSphere is a novel geometry-aware closed-loop simulation framework that captures 2D visual and 3D geometric properties while seamlessly integrating with vision-based end-to-end driving agents.</strong></li>
          </ul>
        </div>
      </div>
      <hr>

      <div class='paper-box'>
        <div class='paper-box-image'>
          <div><div class="badge">ECCV 2024</div>
            <img src='images/merlin.png' alt="sym" style="width: 300px;" />
          </div>
        </div>
        <div class='paper-box-text'>
          <strong>Merlin: Empowering Multimodal LLMs with Foresight Minds</strong><br>
          | ECCV 2024 | 
          <a href="https://arxiv.org/abs/2312.00589" target="_blank">Paper</a> | 
          <a href="https://github.com/Ahnsun/merlin" target="_blank">Code</a> |<br>
          En Yu, Liang Zhao, Yana Wei, Jinrong Yang, Dongming Wu, Lingyu Kong, Haoran Wei, Tiancai Wang, Zheng Ge, Xiangyu Zhang, Wenbing Tao<br>
          <ul>
            <li><strong style="color:green; font-size: 0.8rem;">Merlin is a groundbreaking model capable of generating natural language responses that are intricately linked with object trajectories of multiple images.</strong></li>
          </ul>
        </div>
      </div>
      <hr>
      

      
      <div class='paper-box'>
        <div class='paper-box-image'>
          <div><div class="badge">ICLR 2024</div>
            <img src='images/topomlp.jpg' alt="sym"  style="width: 300px;" />
          </div>
        </div>
        <div class='paper-box-text'>
          <strong>TopoMLP: A Simple yet Strong Pipeline for Driving Topology Reasoning</strong><br>
          | ICLR 2024 | 
          <a href="https://arxiv.org/pdf/2310.06753.pdf" target="_blank">Paper</a> | 
          <a href="https://github.com/wudongming97/TopoMLP" target="_blank">Code</a> |
          <br>
          Dongming Wu, Jiahao Chang, Fan Jia, Yingfei Liu, Tiancai Wang, Jianbing Shen<br>
          <ul>
            <li><strong style="color:green; font-size: 0.8rem;">TopoMLP is the 1st solution for 1st OpenLane Topology in Autonomous Driving Challenge. It suggests a first-detect-then-reason philosophy for better topology prediction.</strong></li>
          </ul>
        </div>
      </div>
      <hr>
      

      
      <div class='paper-box'>
        <div class='paper-box-image'>
          <div><div class="badge">ICCV 2023</div>
            <img src='images/onlinerefer.jpg' alt="sym"  style="width: 300px;" />
          </div>
        </div>
        <div class='paper-box-text'>
          <strong>OnlineRefer: A Simple Online Baseline for Referring Video Object Segmentation</strong><br>
          | ICCV 2023 | 
          <a href="https://arxiv.org/abs/2307.09356" target="_blank">Paper</a> | 
          <a href="https://github.com/wudongming97/OnlineRefer" target="_blank">Code</a> |
          <br>
          Dongming Wu, Tiancai Wang, Yuang Zhang, Xiangyu Zhang, Jianbing Shen<br>
          <ul>
            <li><strong style="color:green; font-size: 0.8rem;">OnlineRefer is the first to challenge the widespread belief that only offline models can deal well with RVOS and makes online RVOS great again.</strong></li>
          </ul>
        </div>
      </div>
      <hr>
      
      <div class='paper-box'>
        <div class='paper-box-image'>
          <div><div class="badge">CVPR 2023</div>
            <img src='images/rmot.jpg' alt="sym"  style="width: 300px;" />
          </div>
        </div>
        <div class='paper-box-text'>
          <strong>Referring Multi-Object Tracking</strong><br>
          | CVPR 2023 | 
          <a href="https://arxiv.org/abs/2303.03366" target="_blank">Paper</a> | 
          <a href="https://github.com/wudongming97/RMOT" target="_blank">Code</a> |
          <br>
          Dongming Wu, Wencheng Han, Tiancai Wang, Xingping Dong, Xiangyu Zhang, Jianbing Shen<br>
          <ul>
            <li><strong style="color:green; font-size: 0.8rem;">RMOT is a new referring understanding task that can detect and track an arbitrary number of objects following human instruction. We propose the first RMOT benchmark Refer-KITTI, and a baseline model TransRMOT.</strong></li>
          </ul>
        </div>
      </div>
      <hr>
      
      <div class='paper-box'>
        <div class='paper-box-image'>
          <div><div class="badge">CVPR 2022</div>
            <img src='images/msla.jpg' alt="sym"  style="width: 300px;" />
          </div>
        </div>
        <div class='paper-box-text'>
          <strong>Multi-Level Representation Learning with Semantic Alignment for Referring Video Object Segmentation</strong><br>
          | CVPR 2022 | 
          <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_Multi-Level_Representation_Learning_With_Semantic_Alignment_for_Referring_Video_Object_CVPR_2022_paper.pdf" target="_blank">Paper</a> |
          <br>
          Dongming Wu, Xingping Dong, Ling Shao, Jianbing Shen
        </div>
      </div>
      <hr>
      <br>
      
      <h3>Journal Papers:</h3>
      <ul>
      <li>
        <strong>Person re-identification by context-aware part attention and multi-head collaborative learning</strong> (<strong>TIFS</strong>)<br>
        Dongming Wu, Mang Ye, Gaojie Lin, Xin Gao, Jianbing Shen<br>
        | 2021 | 
        <a href="https://repository.kaust.edu.sa/bitstream/handle/10754/668975/tifs21cpa-vrid-v5_Xin-comments%20deleted.pdf?sequence=4" target="_blank">Paper</a> |
      </li>
      <hr>


    <li>
      <strong>Reducing estimation bias via triplet-average deep deterministic policy gradient</strong> (<strong>TNNLS</strong>)<br>
      Dongming Wu, Xingping Dong, Jianbing Shen, Steven CH Hoi<br>
      | 2020 | 
      <a href="https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=6923&context=sis_research" target="_blank">Paper</a> |
    </li>
    <hr>
    </ul>
    <br>



    <h3>Technical Report:</h3>
    <ul>
      <li>
        <strong>The 1st-place Solution for CVPR 2023 OpenLane Topology in Autonomous Driving Challenge</strong><br>
        Dongming Wu, Fan Jia, Jiahao Chang, Zhuoling Li, Jianjian Sun, Chunrui Han, Shuailin Li, Yingfei Liu, Zheng Ge, Tiancai Wang<br>
        | 2023 | 
        <a href="https://arxiv.org/pdf/2306.09590.pdf" target="_blank">Paper</a> | 
        <a href="https://github.com/wudongming97/TopoMLP" target="_blank">Code</a> |
      </li>
      <hr>
    </ul>


      

    </section>

    <section id="honors">
      <h2>Honors & Awards</h2>
      <ul>
        <li>Excellent Doctoral Dissertation Beijing Institute of Technology (Âåó‰∫¨ÁêÜÂ∑•Â§ßÂ≠¶‰ºòÁßÄÂçöÂ£´ËÆ∫Êñá)</li>
        <li>Outstanding Graduates of Beijing (Âåó‰∫¨Â∏Ç‰ºòÁßÄÊØï‰∏öÁîü)</li>
        <li>Outstanding Graduates of Beijing Institute of Technology (Âåó‰∫¨ÁêÜÂ∑•Â§ßÂ≠¶‰ºòÁßÄÊØï‰∏öÁîü)</li>
        <li>Excellent Doctoral Dissertation Seedling Fund of Beijing Institute of Technology (Âåó‰∫¨ÁêÜÂ∑•Â§ßÂ≠¶‰ºòÁßÄÂçöÂ£´ËÆ∫ÊñáËÇ≤ËãóÂü∫Èáë)</li>
        <li>National Scholarship (ÂõΩÂÆ∂Â•ñÂ≠¶Èáë), Ministry of Education China</li>
        <li>The 1st place at OpenLane Topology in CVPR2023 Autonomous Driving Challenge ($15,000), Shanghai AI Lab and Huawei</li>
        <li>ChinaCentury Scholarship (ÂçéÁëû‰∏ñÁ∫™Â•ñÂ≠¶Èáë), Beijing Institute of Technology</li>
      </ul>
    </section>

    <section id="service">
      <h2>Service</h2>
      <h3>Conferences:</h3>
      <ul>
        <li>CVPR 2023, 2024, 2025</li>
        <li>ICCV 2023, 2025</li>
        <li>ECCV 2024</li>
        <li>NeurIPS 2025</li>
        <li>ICLR 2025</li>
        <li>AAAI 2025</li>
      </ul>

      <h3>Journals:</h3>
      <ul>
        <li>International Journal of Computer Vision (IJCV)</li>
        <li>IEEE Transactions on Image Processing (TIP)</li>
        <li>IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</li>
        <li>IEEE Transactions on Multimedia (TMM)</li>
        <li>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</li>
        <li>IEEE Transactions on Intelligent Vehicles (TIV)</li>
        <li>Pattern Recognition (PR)</li>
        <li>Neurocomputing</li>
      </ul>
    </section>

    <footer>
      This page was generated in twenty minutes with ChatGPT.
    </footer>
  </div>
</body>
</html>
