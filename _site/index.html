<!doctype html>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin SEO -->





<title>Dongming Wu - Homepage</title>







<meta property="og:locale" content="en">
<meta property="og:site_name" content="Dongming Wu">
<meta property="og:title" content="Dongming Wu">


  <link rel="canonical" href="http://localhost:4000/">
  <meta property="og:url" content="http://localhost:4000/">



  <meta property="og:description" content="">









<!-- end SEO -->


<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="assets/css/main.css">

<meta http-equiv="cleartype" content="on">
<head>
  <base target="_blank">
</head>
    <link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
<link rel="manifest" href="images/site.webmanifest">

<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg masthead__menu-home-item"><a href="#about-me">Homepage</a></li>
          
            <li class="masthead__menu-item"><a href="/#about-me">About Me</a></li>
          
            <li class="masthead__menu-item"><a href="/#-news">News</a></li>
          
            <li class="masthead__menu-item"><a href="/#-experience">Experience</a></li>
          
            <li class="masthead__menu-item"><a href="/#-publications">Publications</a></li>
          
            <li class="masthead__menu-item"><a href="/#-honors">Honors</a></li>
          
            <li class="masthead__menu-item"><a href="/#-service">Service</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div id="main" role="main">
      
  <div class="sidebar sticky">
  

<div itemscope itemtype="http://schema.org/Person" class="profile_box">

  <div class="author__avatar">
    <img src="images/photo.jpg" class="author__avatar" alt="Dongming Wu">
  </div>

  <div class="author__content">
    <h3 class="author__name">Dongming Wu</h3>
    
  </div>

  <div class="author__urls-wrapper">
    <!-- <button class="btn btn--inverse">More Info & Contact</button> -->
    <ul class="author__urls social-icons">
      
        <li><div style="white-space: normal; margin-bottom: 1em; color:gray; font-weight: bold">PhD Student</div></li>
      
<!--      -->
<!--        <li><div style="white-space: normal; margin-bottom: 1em; color:gray; font-weight: bold"></div></li>-->
<!--      -->

      
        <li>Department of Computer Science</li>
      

      
        <li> Beijing Institute of Technology</li>
      

      
      <li><i class="fas fa-fw fa-envelope" aria-hidden="true"></i> Email:</li>
      
        <li><a href="mailto:wudongming97@gmail.com"> wudongming97@gmail.com</a></li>
      
      
      
       
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://github.com/https://github.com/wudongming97"><i class="fab fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://scholar.google.com/citations?user=ejFCAq0AAAAJ&hl=zh-CN"><i class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li>
      
      
      
      
      
    </ul>

    <ul class="author__urls social-icons">
      <h4>Research Areas:</h4>
      <li></li>

      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Beijing</li>
      
    </ul>


      <div class="author__urls_sm">
      
      
        <a href="mailto:wudongming97@gmail.com"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i></a>
      
      
       
      
      
      
      
      
      
      
      
      
      
        <a href="https://github.com/https://github.com/wudongming97"><i class="fab fa-fw fa-github" aria-hidden="true"></i></a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <a href="https://scholar.google.com/citations?user=ejFCAq0AAAAJ&hl=zh-CN"><i class="fas fa-fw fa-graduation-cap"></i></a>
      
      
      
      
      
    </div>
  </div>
</div>

  
  </div>

    
      <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
        <meta itemprop="headline" content="Dongming Wu">
        <div class="page__inner-wrap">
          <section class="page__content" itemprop="text">
            
<p><span class="anchor" id="about-me"></span></p>

<h1 id="about-me">About Me</h1>
<p>I will join in  Multimedia Lab (MMLab) at Chinese University of Hong Kong as a Postdoctoral Fellow, working with Prof. Xiangyu Yue.</p>

<p>In 2025.06, I received my PhD degree in Department of Computer Science, Beijing Institute of Technology, advised by <a href="https://scholar.google.com/citations?user=_Q3NTToAAAAJ&amp;hl=en">Prof. Jianbing Shen</a>. In 2019.06, I recevied my Bachelor degree from the Class of Xu at the same university.</p>

<p>My current research interests lie in <strong>vision-language learning, multimodal large language models (MLLMs), and embodied agents</strong>.
(1) During my graduate studies, I focused on building intelligent perception models that understand visual and linguistic information.
(2) More recently, I’ve been exploring decision-making systems capable of actively interacting with both humans and dynamic environments.
Ultimately, my goal is to develop human-like agents that can perceive real-world environments and make autonomous decisions, moving us closer to achieving artificial general intelligence (AGI). Two articles that have deeply inspired my thinking are <a href="https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf">The Bitter Lesson</a> and <a href="https://ysymyth.github.io/The-Second-Half/">The Second Half</a>.</p>

<p>I am always open to collaboration and discussions about the latest advancements in the field. Feel free to reach out!</p>

<p><span class="anchor" id="-news"></span></p>

<h1 id="news">News</h1>
<ul>
  <li>2025.06: One paper (RAGNet) is accepted by ICCV2025.</li>
  <li>2025.06: I successfully defense my Ph.D. thesis. I’m awarded Outstanding Graduates of Beijing (北京市优秀毕业生).</li>
  <li>2025.02: One paper (DrivingSphere) is accepted by CVPR2025.</li>
  <li>2024.12: One paper (NuPrompt) is accepted by AAAI2025.</li>
  <li>2024.07: One paper (Merlin) is accepted by ECCV2024.</li>
  <li>2024.05: I’m awarded Excellent Doctoral Thesis Seedling Fund (优秀博士论文育苗基金).</li>
  <li>2024.01: One paper (TopoMLP) is accepted by ICLR2024.</li>
  <li>2023.10: I’m awarded National Scholarship!</li>
</ul>

<p><span class="anchor" id="-experience"></span></p>
<h1 id="experience">Experience</h1>

<ul>
  <li>
    <p>2025.02 – 2025.06, I am a research intern at a robotic start-up, Dexmal, supervised by <a href="https://scholar.google.com.hk/citations?user=pF9KA1sAAAAJ&amp;hl=zh-CN">Yingfei Liu</a> and <a href="https://scholar.google.com.hk/citations?user=YI0sRroAAAAJ&amp;hl=zh-CN">Tiancai Wang</a>.</p>
  </li>
  <li>
    <p>2024.09 – 2025.01, I am a visiting student at MBZUAI, supervised by <a href="https://mbzuai.ac.ae/study/faculty/rao-muhammad-anwer/">Prof. Rao Muhammad Anwer</a> and <a href="https://sites.google.com/view/fahadkhans">Prof. Fahad Shahbaz Khan</a>.</p>
  </li>
  <li>
    <p>2022.06 – 2024.08, I am a research intern at Foundation model group in MEGVII, supervised by <a href="https://scholar.google.com.hk/citations?user=YI0sRroAAAAJ&amp;hl=zh-CN">Tiancai Wang</a> and <a href="https://scholar.google.com/citations?user=yuB-cfoAAAAJ&amp;hl=zh-CN&amp;oi=ao">Xiangyu Zhang</a>.</p>
  </li>
  <li>
    <p>2021.05 – 2022.05, I am a research intern at Inception Institute of Artificial Intelligence, supervised by <a href="https://xingpingdong.github.io/">Xingping Dong</a> and <a href="https://scholar.google.com/citations?user=z84rLjoAAAAJ&amp;hl=zh-CN&amp;oi=ao">Prof. Ling Shao</a>.</p>
  </li>
</ul>

<p><span class="anchor" id="-publications"></span></p>
<h1 id="publications">Publications</h1>

<h2 id="preprint-papers">Preprint Papers:</h2>
<ul>
  <li>
    <p><strong>Grounding Beyond Detection: Enhancing Contextual Understanding in Embodied 3D Grounding</strong><br />
Yani Zhang, Dongming Wu, Hao Shi, Yingfei Liu, Tiancai Wang, Haoqiang Fan, Xingping Dong <br />
<a href="https://huggingface.co/spaces/AGC2024/visual-grounding-2024"><strong style="color:red"> 1st place on EmbodiedScan Visual Grounding</strong></a> <br />
|2025|<a href="https://arxiv.org/abs/2506.05199">Paper</a>|<a href="https://github.com/zyn213/DEGround">Code</a>|</p>
  </li>
  <li>
    <p><strong>Is a 3D-Tokenized LLM the Key to Reliable Autonomous Driving?</strong><br />
Yifan Bai*, Dongming Wu*, Yingfei Liu, Fan Jia, Weixin Mao, Ziheng Zhang, Yucheng Zhao, Jianbing Shen, Xing Wei, Tiancai Wang, Xiangyu Zhang (*Equal Contributions) <br />
|2024|<a href="https://arxiv.org/pdf/2405.18361">Paper</a>|</p>
  </li>
  <li>
    <p><strong>Bootstrapping Referring Multi-Object Tracking</strong><br />
Yani Zhang, Dongming Wu, Wencheng Han, Xingping Dong <br />
|2024|<a href="https://arxiv.org/pdf/2406.05039">Paper</a>|<a href="https://github.com/zyn213/TempRMOT">Code</a>|</p>
  </li>
</ul>

<h2 id="conference-papers">Conference Papers</h2>
<div class="paper-box"><div class="paper-box-image"><div><div class="badge">ICCV 2025</div><img src="images/ragnet.jpg" alt="sym" width="100%" height="100%" /></div></div>
<div class="paper-box-text">

    <p><strong>A Large-Scale Reasoning-based Affordance Segmentation Dataset and Model for Universal Robot Grasping</strong> <br />
|<strong>ICCV 2025</strong>|</p>

    <p>Dongming Wu, Yanping Fu, Saike Huang, Yingfei Liu, Fan Jia, Nian Liu, Feng Dai, Tiancai Wang, Rao Muhammad Anwer, Fahad Shahbaz Khan, Jianbing Shen</p>

    <ul>
      <li><strong style="color:green"> We present a large-scale reasoning-based affordance segmentation benchmark RAGNet and introduce a comprehensive affordance-based grasping framework AffordanceNet.</strong></li>
    </ul>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">CVPR 2025</div><img src="images/drivingsphere.jpg" alt="sym" width="100%" height="100%" /></div></div>
<div class="paper-box-text">

    <p><strong>DrivingSphere: Building a High-fidelity 4D World for Closed-loop Simulation</strong> <br />
|<strong>CVPR 2025</strong>|<a href="https://arxiv.org/abs/2411.11252">Paper</a>|<a href="https://yanty123.github.io/DrivingSphere/">Code</a>|</p>

    <p>Tianyi Yan, Dongming Wu, Wencheng Han, Junpeng Jiang, Xia Zhou, Kun Zhan, Cheng-zhong Xu, Jianbing Shen</p>

    <ul>
      <li><strong style="color:green"> DrivingSphere is a novel geometry-aware closed-loop simulation framework that captures 2D visual and 3D geometric properties while seamlessly integrating with vision-based end-to-end driving agents.</strong></li>
    </ul>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">AAAI 2025</div><img src="images/nuprompt.jpg" alt="sym" width="100%" height="100%" /></div></div>
<div class="paper-box-text">

    <p><strong>Language prompt for autonomous driving</strong> <br />
|<strong>AAAI 2025</strong>|<a href="https://arxiv.org/pdf/2309.04379.pdf">Paper</a>|<a href="https://github.com/wudongming97/Prompt4Driving">Code</a>|</p>

    <p>Dongming Wu, Wencheng Han, Tiancai Wang, Yingfei Liu, Xiangyu Zhang, Jianbing Shen</p>

    <ul>
      <li><strong style="color:green"> NuPrompt is the first dataset specializing in multiple 3D objects of interest from video domain for autonomous driving.</strong></li>
    </ul>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">ECCV 2024</div><img src="images/merlin.png" alt="sym" width="100%" height="100%" /></div></div>
<div class="paper-box-text">

    <p><strong>Merlin:Empowering Multimodal LLMs with Foresight Minds</strong><br />
|<strong>ECCV 2024</strong>|<a href="https://arxiv.org/abs/2312.00589">Paper</a>|<a href="https://github.com/Ahnsun/merlin">Code</a>|</p>

    <p>En Yu, Liang Zhao, Yana Wei, Jinrong Yang, Dongming Wu, Lingyu Kong, Haoran Wei, Tiancai Wang, Zheng Ge, Xiangyu Zhang, Wenbing Tao</p>

    <ul>
      <li><strong style="color:green"> Merlin is a groundbreaking model capable of generating natural language responses that are intricately linked with object trajectories of multiple images.</strong></li>
    </ul>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">ICLR 2024</div><img src="images/topomlp.jpg" alt="sym" width="100%" height="100%" /></div></div>
<div class="paper-box-text">

    <p><strong>TopoMLP: A Simple yet Strong Pipeline for Driving Topology Reasoning</strong><br />
|<strong>ICLR 2024</strong>|<a href="https://arxiv.org/pdf/2310.06753.pdf">Paper</a>|<a href="https://github.com/wudongming97/TopoMLP">Code</a>|</p>

    <p>Dongming Wu, Jiahao Chang, Fan Jia, Yingfei Liu, Tiancai Wang, Jianbing Shen</p>

    <ul>
      <li><strong style="color:green"> TopoMLP is the 1st solution for 1st OpenLane Topology in Autonomous Driving Challenge. It suggests a first-detect-then-reason philosophy for better topology prediction.</strong></li>
    </ul>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">ICCV 2023</div><img src="images/onlinerefer.jpg" alt="sym" width="100%" height="100%" /></div></div>
<div class="paper-box-text">

    <p><strong>OnlineRefer: A Simple Online Baseline for Referring Video Object Segmentation</strong><br />
|<strong>ICCV 2023</strong>|<a href="https://arxiv.org/abs/2307.09356">Paper</a>|<a href="https://github.com/wudongming97/OnlineRefer">Code</a>|</p>

    <p>Dongming Wu, Tiancai Wang, Yuang Zhang, Xiangyu Zhang, Jianbing Shen</p>

    <ul>
      <li><strong style="color:green"> OnlineRefer is the first to challenge the widespread belief that only offline models can deal well with RVOS and makes online RVOS great again.</strong></li>
    </ul>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">CVPR 2023</div><img src="images/rmot.jpg" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><strong>Referring Multi-Object Tracking</strong> <br />
|<strong>CVPR 2023</strong>|<a href="https://arxiv.org/abs/2303.03366">Paper</a>|<a href="https://github.com/wudongming97/RMOT">Code</a>|</p>

    <p>Dongming Wu, Wencheng Han, Tiancai Wang, Xingping Dong, Xiangyu Zhang, Jianbing Shen</p>

    <ul>
      <li><strong style="color:green">RMOT is a new referring understanding task that can detect and track an arbitrary number of objects following human instruction. We propose the first RMOT benckmark Refer-KITTI, and a baseline model TransRMOT.</strong></li>
    </ul>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">CVPR 2022</div><img src="images/msla.jpg" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><strong>Multi-Level Representation Learning with Semantic Alignment for Referring Video Object Segmentation</strong> <br />
|<strong>CVPR 2022</strong>|<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_Multi-Level_Representation_Learning_With_Semantic_Alignment_for_Referring_Video_Object_CVPR_2022_paper.pdf">Paper</a>|</p>

    <p>Dongming Wu, Xingping Dong, Ling Shao, Jianbing Shen</p>

  </div>
</div>

<h2 id="journal-papers">Journal Papers:</h2>

<ul>
  <li>
    <p><strong>Person re-identification by context-aware part attention and multi-head collaborative learning</strong>(<strong>TIFS</strong>)<br />
Dongming Wu, Mang Ye, Gaojie Lin, Xin Gao, Jianbing Shen <br />
|2021|<a href="https://repository.kaust.edu.sa/bitstream/handle/10754/668975/tifs21cpa-vrid-v5_Xin-comments%20deleted.pdf?sequence=4">Paper</a>|</p>
  </li>
  <li>
    <p><strong>Reducing estimation bias via triplet-average deep deterministic policy gradient</strong>(<strong>TNNLS</strong>) <br />
Dongming Wu, Xingping Dong, Jianbing Shen, Steven CH Hoi <br />
|2020|<a href="https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=6923&amp;context=sis_research">Paper</a>|</p>
  </li>
</ul>

<h2 id="technical-report">Technical Report:</h2>

<ul>
  <li><strong>The 1st-place Solution for CVPR 2023 OpenLane Topology in Autonomous Driving Challenge</strong><br />
Dongming Wu, Fan Jia, Jiahao Chang, Zhuoling Li, Jianjian Sun, Chunrui Han, Shuailin Li, Yingfei Liu, Zheng Ge, Tiancai Wang<br />
|2023|<a href="https://arxiv.org/pdf/2306.09590.pdf">Paper</a>|<a href="https://github.com/wudongming97/TopoMLP">Code</a>|</li>
</ul>

<p><span class="anchor" id="-honors"></span></p>
<h1 id="honors">Honors</h1>

<ul>
  <li>Outstanding Graduates of Beijing (北京市优秀毕业生).</li>
  <li>Outstanding Graduates of BIT (北京理工大学优秀毕业生), Beijing Institute of Technology.</li>
  <li>Excellent Doctoral Thesis Seedling Fund(优秀博士论文育苗基金), Beijing Institute of Technology.</li>
  <li>National Scholarship, Ministry of Education China.</li>
  <li>The 1st place at <a href="https://opendrivelab.com/AD23Challenge.html#openlane_topology">OpenLane Topology</a> in CVPR2023 Autonomous Driving Challenge ($15,000), Shanghai AI Lab and Huawei.</li>
  <li>ChinaCentury(华瑞世纪) Scholarship, Beijing Institute of Technology.</li>
</ul>

<p><span class="anchor" id="-service"></span></p>
<h1 id="service">Service</h1>
<p>Invited Reviewer for conferences:</p>
<ul>
  <li>CVPR 2023,2024,2025</li>
  <li>ICCV 2023,2025</li>
  <li>ECCV 2024</li>
  <li>ICLR 2025</li>
  <li>AAAI 2025</li>
</ul>

<p>Invited Reviewer for journals:</p>
<ul>
  <li>International Journal of Computer Vision (IJCV)</li>
  <li>IEEE Transactions on Image Processing (TIP)</li>
  <li>IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</li>
  <li>IEEE Transactions on Multimedia (TMM)</li>
  <li>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</li>
  <li>IEEE Transactions on Intelligent Vehicles (TIV)</li>
  <li>Pattern Recognition (PR)</li>
  <li>Neurocomputing</li>
</ul>


          </section>
        </div>
      </article>
    </div>

    <script src="assets/js/main.min.js"></script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id="></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', "");
</script>


<script>
    $(document).ready(function () {
        
        var gsDataBaseUrl = 'https://cdn.jsdelivr.net/gh/wudongming97/homepage@'
        
        $.getJSON(gsDataBaseUrl + "google-scholar-stats/gs_data.json", function (data) {
            var totalCitation = data['citedby']
            var hindex = data['hindex']
            document.getElementById('total_cit').innerHTML = totalCitation;
            document.getElementById('hindex').innerHTML = hindex;
            var citationEles = document.getElementsByClassName('show_paper_citations')
            Array.prototype.forEach.call(citationEles, element => {
                var paperId = element.getAttribute('data')
                var numCitations = data['publications'][paperId]['num_citations']
                element.innerHTML = '| Citations: ' + numCitations;
            });
        });
    })
</script>

    <footer style="text-align:center; height:20px; color:#555 ">&copy; Copyright 2023 Dongming Wu</footer>
    <br/>
  </body>
</html>
